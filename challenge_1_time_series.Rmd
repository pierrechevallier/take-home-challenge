---
title: "Challenge 1 - Time series"
author: "Pierre Chevallier"
date: "1/24/2021"
knit: (function(inputFile, encoding) { 
        out_dir <- 'output';
        rmarkdown::render(
          inputFile,
          encoding=encoding,
          output_file=file.path(
            dirname(inputFile),
            out_dir,
            'challenge_1_time_series.html'
          )
        )
      })
output: html_document
params:
  path: "~/Documents/R/Take_home_blog/take-home-challenge/"
---

# TL;DR

We have to predict ice cream production from a dataset provided by [CSV](https://github.com/pierrechevallier/take-home-challenge/blob/main/Take_home_1/Challenge%201_cadairydata.csv). The ice cream shows a strong seasonality (obviously people eat ice cream in summer more than winter!) and makes it easier to predict once you have figured out the seasonal component.

# Setup

The setup consists of importing the data, and loading the libraries for the analysis.
We will work using this RMarkdown document to include the findings of our research.
This is for 2 reasons:

  - Making the document easily maintainable with one place for the writing and the coding (especially to include it later on the blog)
  - To share it and export it easily so it can be hosted on GitHub with the code and the findings

I attached in quote excerpts from the challenge requirements to make sure to keep track of the end goal.

Also we will use the packages:

 - `Dplyr` for easier manipulation, this is a must have for R (maybe I'll do a topic on that)
 - `zoo` for the year  month handling
 - `ggplot2` for generating charts
 - `patchwork` for the layout of the ggplot charts, [reference](https://github.com/thomasp85/patchwork)
 - `tseries` for tools for time series like KPSS tests
 - `astsa` for forecasting

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(zoo)
library(ggplot2)
library(patchwork)
library(tseries)
library(astsa)
# Functions ----
source_files <- list.files(path=paste0(params$path,"scripts/"), pattern = ".*R")
for (file in source_files) {
  source(paste0(params$path,"/scripts/", file)) 
}
# Data ----
dairy_data <- read.csv(paste0(params$path,"Take_home_1/Challenge 1_cadairydata.csv"))
```

> In this challenge, you will create a forecasting model for dairy production. The forecasting model is based on an existing dataset of dairy production history for California.
> 
> - Open Challenge 1_cadairydata.csv file. 
> 
> - View the contents of the `cadairydata.csv` file, noting that it contains dairy production data from January 1995 to December 2013

Here we are, the data have been loaded just before, so we can have a quick look at it.

```{r Content view}
head(dairy_data)
str(dairy_data)
```

Right now we can see a few things that are interesting, first of all, the data are already converted into a year-month format (numerical format with the year as the integer and the month as a decimal value). Also there are 4 series of data in our data set. They are all fine, but not very easy to use when typing code, let's rename them:

 - Cotagecheese.Prod -> CottageCheeseProd (typo for Cottage cheese !)
 - Icecream.Prod -> IceCreamProd
 - Milk.Prod -> MilkProd
 - N.CA.Fat.Price -> FatPrice

```{r Renaming}
dairy_data <- dairy_data %>% 
  rename(
    CottageCheeseProd = Cotagecheese.Prod,
    IceCreamProd = Icecream.Prod,
    MilkProd = Milk.Prod,
    FatPrice = N.CA.Fat.Price,
    YearMonth = Year.Month,
    MonthNumber = Month.Number
  ) %>% 
  mutate(
    YearMonth = as.yearmon(paste(Year, MonthNumber, "1", sep = "-")),
    Date = as.Date(YearMonth)
  )
```

# 1. Patterns in the data

> Identify any patterns in the data if any

We have overall 4 main variables, which we renamed, to be analysed, we first can look at them individually to look in the plot for:

 - Trend
 - Seasonality
 
And then we can check them between each other:

 - Are they correlated ?
 - Are they not correlated ?
 
Of course, this will yield further questions which we can solve with statistical tests to check for example for [stationarity](https://en.wikipedia.org/wiki/Stationary_process). But first, let's plot the data and see how they look throughout time.

```{r Plotting and pattern in data}
# CotageCheeseProd ----
cot_plot <- buildPlot(dairy_data, x = "Date", y = "CottageCheeseProd")
# IceCreamProd ----
ice_plot <- buildPlot(dairy_data, x = "Date", y = "IceCreamProd")
# IceCreamProd ----
milk_plot <- buildPlot(dairy_data, x = "Date", y = "MilkProd")
# IceCreamProd ----
fat_plot <- buildPlot(dairy_data, x = "Date", y = "FatPrice")

# Displaying the data together
(cot_plot / ice_plot / milk_plot / fat_plot)
```

Ok now that is interesting, let's sum up what we see from here. I will not dwell on the concepts of trends, seasonality, stationarity and [heteroscedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity). These will maybe be covered in another topic, so let's sum up what we see for each variables:

|  Variable|  Property|  Compared to other variables|
|--:|--:|--:|
|  Cottage cheese production|  A trend, and a non-stationary process, but with stochastic component|  Could be correlated to milk production|
|  Ice Cream Production|  A very seasonal product (no wonder, ice cream sales tend to be in summer)|  NA|
|  Milk Production|  A clear trend and maybe a bit of seasonality|  Could be correlated to Cottage cheese production|
|  Fat Price|  An heteroscedastic process with a trend|  NA|

As a matter of fact there is a significant autocorrelation of the variables (\code{CottageCheeseProd}, \code{IceCreamProd} and \code{MilkProd}).


Now in term of cross-correlation of the variables, I have found [this link that provides details on how to do and use it in R](https://medium.com/@jan.seifert/the-r-cross-correlation-function-f5f426006425). I can refer also to the well documented [article on cross-correlation on Wikipedia](https://en.wikipedia.org/w/index.php?title=Cross-correlation&oldid=936791961) that can help understand better how it works.

Since the ice cream production is the one that should be predicted in this analysis, let's focus on that variable. Let's find out if it's a stationary process by using an [Augmented Dickey-Fuller (ADF)](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) test as well as a [KPSS test](https://en.wikipedia.org/wiki/KPSS_test), for better understanding on how they work, you can have a look at this [link for a practical example](https://www.projectguru.in/what-is-a-stationarity-test-how-to-do-it/). 

```{r Stationarity}
kpss.test(dairy_data$IceCreamProd)
adf.test(dairy_data$IceCreamProd)
```

Let's resume the results:

|  Test|  p-value|  Test statistic|
|--:|--:|--:|
|  ADF|  0.01|  -10.376|
|  KPSS|  0.086|  0.37796|

So let's recap, we reject the null hypothesis for the ADF test (there is no unit root, thus probably stationarity), and accept the null hypothesis for the KPSS test: the series is stationary without unit root.

Since we have a stationary process, we can use the Partial ACF to estimate the seasonal component

```{r Seasonality}
print(acf(dairy_data$IceCreamProd))
print(pacf(dairy_data$IceCreamProd))
```

Oh oh oh, interesting, we have some seasonality (though we new it beforehand of course), it seems to be especially present on the 2nd lag and on the 12th lag: autocorrelation and annual seasonality. Fair enough !

To put it simply, this is a series without really a trend, so we will just have to predict the seasonal component of the series and predict the future values.

Let's convert the couple \code{YearMonth} and \code{IceCreamProd} into a time series object.

```{r Time series conversion and seasonal component estimation}
ts_ice_cream <- ts(dairy_data$IceCreamProd, start=min(dairy_data$YearMonth), end = max(dairy_data$YearMonth), frequency = 12)
```

# 2. Forecasting the series

> Based on above, create a forecasting model and predict the next 12 months ice cream production and plot the time series using any of the following desired tools (e.g. R, Python, Qlik, PBI, Tableau...) 
> 
> | Year | Month | MonthNumber | Ice cream production |
> | ---- | ----- | ----------- | -------------------- |
> | 2014 | Jan   | 1           |                      |
> | 2014 | Feb   | 2           |                      |
> | 2014 | Mar   | 3           |                      |
> | 2014 | Apr   | 4           |                      |
> | 2014 | May   | 5           |                      |
> | 2014 | Jun   | 6           |                      |
> | 2014 | Jul   | 7           |                      |
> | 2014 | Aug   | 8           |                      |
> | 2014 | Sep   | 9           |                      |
> | 2014 | Oct   | 10          |                      |
> | 2014 | Nov   | 11          |                      |
> | 2014 | Dec   | 12          |                      |

Here we are in the more interesting part of the problem, the forecasting. Probably here, the best approach will be to use a [seasonal autoregressive integrated moving average](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model (or seasonal ARIMA or SARIMA). We need to estimate several components of the model:

 - p, which is the order or number of time lags
 - d, the differencing or how lagged are the data
 - q, the order of the moving average part of the model
 - P, the order or number of time lags of the seasonal component
 - D, the differencing or how lagged the seasonal component is
 - Q, the order of the moving average of the seasonal component

The best approach to estimate these parameters is to use the [Box-Jenkins method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method). We of course already checked a few of these details beforehand ! [Another interesting post](https://medium.com/towards-artificial-intelligence/statistical-modeling-of-time-series-data-part-3-forecasting-stationary-time-series-using-sarima-f0ff1284bebb) to have an idea about how to estimate each parameter.

 - So, given the PACF we can conclude that \code{p=4}. This is the order when the lags are within the significant order, which is the case at the point 4.

 - We have determined that the series is already stationary, thus \code{d=0}, we won't need to remove the trend associated with it.

 - And given the ACF we can conclude that \code{q=3}. The approach is similar to the one for the PACF, except with the ACF. Given several iteration of the model we finally settle on 1, which has the smallest AIC and BIC criteria.
 
 - We settle, based upon the PACF and ACF on the following values for \code{P=1} and \code{Q=4}, the seasonal component being \code{S=12} since we have a 12 month seasonality.

```{r Fitting of SARIMA model}
sarima_fit <- sarima(ts_ice_cream, p= 4, d = 0, q = 1, P= 1, D = 0, Q = 4, S = 12)
```

The output looks rather ok and we have a BIC and AIC that are quite manageable.

Given the table of the parameters we could tweak it out a bit and change the \code{p}parameter to 3, but it doesn't gain much more accuracy, the computation time is longer and the model fitting doesn't converge fast enough.


>       Estimate     SE  t.value p.value
> ar1    -0.5882 0.0799  -7.3599  0.0000
> ar2     0.4113 0.0779   5.2793  0.0000
> ar3     0.3600 0.0772   4.6612  0.0000
> ar4     0.1117 0.0769   1.4529  0.1477
> ma1     0.9587 0.0283  33.8715  0.0000
> sar1    0.9904 0.0059 168.8495  0.0000
> sma1   -0.4795 0.0752  -6.3756  0.0000
> sma2   -0.2259 0.0773  -2.9239  0.0038
> sma3   -0.0815 0.0867  -0.9404  0.3481
> sma4    0.2305 0.0868   2.6564  0.0085
> xmean  70.2552 6.7042  10.4793  0.0000

Now that the model is fitted we can start using it to predict the next 12 months of production.

``` {r Prediction of next 12 months}
predictions <- sarima.for(ts_ice_cream, n.ahead=12, p= 4, d = 0, q = 1, P= 1, D = 0, Q = 4, S = 12, plot.all = T)
```

# 3. Additional insights, findings and accuracy tracking

> Explain to us any additional insights that you discovered and tell us how would you track accuracy forward looking?
